{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean', weights='uniform'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Stores the training data and labels.\n",
    "        \"\"\"\n",
    "        self.X_train = np.array(X)  # Ensure X_train is a NumPy array\n",
    "        self.y_train = np.array(y)  # Ensure y_train is a NumPy array\n",
    "\n",
    "    def compute_distance(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Computes the distance between each row in X1 (training data) and X2 (a single test sample).\n",
    "        \"\"\"\n",
    "        X1 = np.array(X1, dtype=np.float64)\n",
    "        X2 = np.array(X2, dtype=np.float64)\n",
    "\n",
    "        if X2.ndim == 0 or X2.shape == ():\n",
    "            X2 = np.array([X2])\n",
    "\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((X1 - X2.reshape(1, -1)) ** 2, axis=1))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(X1 - X2.reshape(1, -1)), axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance metric\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for the given test data.\n",
    "        \"\"\"\n",
    "        X = np.array(X, dtype=np.float64)\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = self.compute_distance(self.X_train, x)\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = self.y_train[k_indices]\n",
    "\n",
    "            if self.weights == 'uniform':\n",
    "                # Majority vote for uniform weighting\n",
    "                unique, counts = np.unique(k_nearest_labels, return_counts=True)\n",
    "                predictions.append(unique[np.argmax(counts)])\n",
    "            elif self.weights == 'distance':\n",
    "                # Weighting by distance for distance-based weighting\n",
    "                k_nearest_distances = distances[k_indices]\n",
    "                inverse_distances = 1 / (k_nearest_distances + 1e-5)  # Avoid division by zero\n",
    "                weighted_vote = {}\n",
    "                for i, label in enumerate(k_nearest_labels):\n",
    "                    if label in weighted_vote:\n",
    "                        weighted_vote[label] += inverse_distances[i]\n",
    "                    else:\n",
    "                        weighted_vote[label] = inverse_distances[i]\n",
    "                predictions.append(max(weighted_vote, key=weighted_vote.get))\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported weighting scheme\")\n",
    "        \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data):\n",
    "    # Concatenate train and test data for consistent preprocessing\n",
    "    all_data = pd.concat([train_data, test_data], axis=0, sort=False)\n",
    "\n",
    "    # Handle categorical variables\n",
    "    all_data = pd.get_dummies(all_data, columns=['Geography', 'Gender'], drop_first=True)\n",
    "\n",
    "    # Select features for model\n",
    "    features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', \n",
    "                'IsActiveMember', 'EstimatedSalary', 'Geography_Germany', 'Geography_Spain', 'Gender_Male']\n",
    "\n",
    "    # Impute missing values with median\n",
    "    all_data[features] = all_data[features].fillna(all_data[features].median())\n",
    "\n",
    "    # Scale features\n",
    "    for feature in features:\n",
    "        all_data[feature] = (all_data[feature] - all_data[feature].mean()) / all_data[feature].std()\n",
    "\n",
    "    # Split back into train and test\n",
    "    X = all_data[features].iloc[:len(train_data)].values\n",
    "    y = train_data['Exited'].astype(int).values\n",
    "    X_test = all_data[features].iloc[len(train_data):].values\n",
    "\n",
    "    return X, y, X_test, all_data['CustomerId'].iloc[len(train_data):].values, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(X, y, knn, n_splits=5):\n",
    "    # Ensure reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    indices = np.random.permutation(len(X))\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    # Determine fold size\n",
    "    fold_size = len(X) // n_splits\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size if i < n_splits - 1 else len(X)\n",
    "\n",
    "        # Create validation and training sets\n",
    "        X_val = X[start:end]\n",
    "        y_val = y[start:end]\n",
    "\n",
    "        X_train = np.concatenate([X[:start], X[end:]], axis=0)\n",
    "        y_train = np.concatenate([y[:start], y[end:]], axis=0)\n",
    "\n",
    "        # Fit the KNN model on the training fold\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the validation fold\n",
    "        y_pred = knn.predict(X_val)\n",
    "\n",
    "        # Compute accuracy for the current fold\n",
    "        accuracy = np.mean(y_pred == y_val)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # Return the average accuracy and all accuracies across folds\n",
    "    return np.mean(accuracies), accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hyperparameter_tuning(X, y):\n",
    "    best_k = None\n",
    "    best_metric = None\n",
    "    best_weight = None\n",
    "    best_accuracy = -1\n",
    "    results = []\n",
    "\n",
    "    # Define ranges of k values and distance metrics to try\n",
    "    k_values = [15,18,20,23,25]  # Example: Test k values from 10 to 20\n",
    "    distance_metrics = ['euclidean', 'manhattan']  # You can test multiple distance metrics\n",
    "    weights_options = ['uniform', 'distance']  # Test both uniform and distance-based weighting\n",
    "\n",
    "    # Iterate over each combination of k, distance metric, and weighting\n",
    "    for k in k_values:\n",
    "        for metric in distance_metrics:\n",
    "            for weight in weights_options:\n",
    "                print(f\"Testing k={k}, distance_metric={metric}, weight={weight}...\")  # Print current step\n",
    "\n",
    "                knn = KNN(k=k, distance_metric=metric, weights=weight)\n",
    "                average_accuracy, _ = cross_validate(X, y, knn, n_splits=10)  # Perform cross-validation\n",
    "\n",
    "                # Print the accuracy for this configuration\n",
    "                print(f\"Accuracy for k={k}, distance_metric={metric}, weight={weight}: {average_accuracy:.4f}\")\n",
    "\n",
    "                # Append results for each combination\n",
    "                results.append((k, metric, weight, average_accuracy))\n",
    "\n",
    "                # Check if this is the best accuracy so far\n",
    "                if average_accuracy > best_accuracy:\n",
    "                    best_accuracy = average_accuracy\n",
    "                    best_k = k\n",
    "                    best_metric = metric\n",
    "                    best_weight = weight\n",
    "                    print(f\"New best accuracy: {best_accuracy:.4f} with k={k}, distance_metric={metric}, weight={weight}\")\n",
    "\n",
    "    return best_k, best_metric, best_weight, best_accuracy, results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing k=15, distance_metric=euclidean, weight=uniform...\n",
      "Accuracy for k=15, distance_metric=euclidean, weight=uniform: 0.8765\n",
      "New best accuracy: 0.8765 with k=15, distance_metric=euclidean, weight=uniform\n",
      "Testing k=15, distance_metric=euclidean, weight=distance...\n",
      "Accuracy for k=15, distance_metric=euclidean, weight=distance: 0.8780\n",
      "New best accuracy: 0.8780 with k=15, distance_metric=euclidean, weight=distance\n",
      "Testing k=15, distance_metric=manhattan, weight=uniform...\n",
      "Accuracy for k=15, distance_metric=manhattan, weight=uniform: 0.8779\n",
      "Testing k=15, distance_metric=manhattan, weight=distance...\n",
      "Accuracy for k=15, distance_metric=manhattan, weight=distance: 0.8788\n",
      "New best accuracy: 0.8788 with k=15, distance_metric=manhattan, weight=distance\n",
      "Testing k=18, distance_metric=euclidean, weight=uniform...\n",
      "Accuracy for k=18, distance_metric=euclidean, weight=uniform: 0.8761\n",
      "Testing k=18, distance_metric=euclidean, weight=distance...\n",
      "Accuracy for k=18, distance_metric=euclidean, weight=distance: 0.8770\n",
      "Testing k=18, distance_metric=manhattan, weight=uniform...\n",
      "Accuracy for k=18, distance_metric=manhattan, weight=uniform: 0.8759\n",
      "Testing k=18, distance_metric=manhattan, weight=distance...\n",
      "Accuracy for k=18, distance_metric=manhattan, weight=distance: 0.8803\n",
      "New best accuracy: 0.8803 with k=18, distance_metric=manhattan, weight=distance\n",
      "Testing k=20, distance_metric=euclidean, weight=uniform...\n",
      "Accuracy for k=20, distance_metric=euclidean, weight=uniform: 0.8769\n",
      "Testing k=20, distance_metric=euclidean, weight=distance...\n",
      "Accuracy for k=20, distance_metric=euclidean, weight=distance: 0.8771\n",
      "Testing k=20, distance_metric=manhattan, weight=uniform...\n",
      "Accuracy for k=20, distance_metric=manhattan, weight=uniform: 0.8761\n",
      "Testing k=20, distance_metric=manhattan, weight=distance...\n",
      "Accuracy for k=20, distance_metric=manhattan, weight=distance: 0.8789\n",
      "Testing k=23, distance_metric=euclidean, weight=uniform...\n",
      "Accuracy for k=23, distance_metric=euclidean, weight=uniform: 0.8757\n",
      "Testing k=23, distance_metric=euclidean, weight=distance...\n",
      "Accuracy for k=23, distance_metric=euclidean, weight=distance: 0.8760\n",
      "Testing k=23, distance_metric=manhattan, weight=uniform...\n",
      "Accuracy for k=23, distance_metric=manhattan, weight=uniform: 0.8762\n",
      "Testing k=23, distance_metric=manhattan, weight=distance...\n",
      "Accuracy for k=23, distance_metric=manhattan, weight=distance: 0.8801\n",
      "Testing k=25, distance_metric=euclidean, weight=uniform...\n",
      "Accuracy for k=25, distance_metric=euclidean, weight=uniform: 0.8738\n",
      "Testing k=25, distance_metric=euclidean, weight=distance...\n",
      "Accuracy for k=25, distance_metric=euclidean, weight=distance: 0.8762\n",
      "Testing k=25, distance_metric=manhattan, weight=uniform...\n",
      "Accuracy for k=25, distance_metric=manhattan, weight=uniform: 0.8765\n",
      "Testing k=25, distance_metric=manhattan, weight=distance...\n",
      "Accuracy for k=25, distance_metric=manhattan, weight=distance: 0.8778\n",
      "Best k: 18\n",
      "Best Distance Metric: manhattan\n",
      "Best Weighting Scheme: distance\n",
      "Best Accuracy: 0.8803333333333333\n",
      "Test predictions saved to 'submissions.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Preprocess the data, unpack all the returned values\n",
    "X, y, X_test, test_customer_ids, features = preprocess_data(train_df, test_df)\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "best_k, best_metric, best_weight, best_accuracy, results = hyperparameter_tuning(X, y)\n",
    "\n",
    "print(\"Best k:\", best_k)\n",
    "print(\"Best Distance Metric:\", best_metric)\n",
    "print(\"Best Weighting Scheme:\", best_weight)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "knn_optimal = KNN(k=best_k, distance_metric=best_metric, weights=best_weight)\n",
    "knn_optimal.fit(X, y)\n",
    "\n",
    "# Make final predictions on the test data\n",
    "test_predictions = knn_optimal.predict(X_test)\n",
    "\n",
    "# Create a submission DataFrame\n",
    "submission_df = pd.DataFrame({'CustomerId': test_customer_ids, 'Exited': test_predictions})\n",
    "submission_df.to_csv('submissions.csv', index=False)\n",
    "print(\"Test predictions saved to 'submissions.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to 'submissions.csv'.\n"
     ]
    }
   ],
   "source": [
    "knn = KNN(k=best_k, distance_metric=best_metric)\n",
    "knn.fit(X, y)\n",
    "test_predictions = knn.predict(X_test)\n",
    "\n",
    "test_ids = pd.read_csv('test.csv')['id']\n",
    "\n",
    "# Create a DataFrame with the 'id' and 'Exited' predictions\n",
    "submission_df = pd.DataFrame({'id': test_ids, 'Exited': test_predictions})\n",
    "\n",
    "# Save the predictions to a CSV file for submission\n",
    "submission_df.to_csv('submissions.csv', index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Test predictions saved to 'submissions.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to 'submissions.csv'.\n"
     ]
    }
   ],
   "source": [
    "if submission_df['id'].duplicated().any():\n",
    "    raise ValueError(\"Duplicate CustomerId values found in submission!\")\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('submissions.csv', index=False)\n",
    "print(\"Test predictions saved to 'submissions.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
